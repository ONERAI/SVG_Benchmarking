Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 10
Rules claiming more threads will be scaled down.
Job stats:
job             count    min threads    max threads
------------  -------  -------------  -------------
real_data           1              1              1
run_GPcounts        6             10             10
total               7              1             10

Select jobs to execute...

[Wed Jul  5 16:09:54 2023]
rule run_GPcounts:
    input: /data/pinello/PROJECTS/2023_03_SVGBenchmarking/HumanHeart/AnnData/processed_data/FZ_ACH006.h5ad
    output: results/human_heart/GPcounts/FZ_ACH006.csv
    jobid: 4
    benchmark: benchmarks/human_heart/GPcounts/FZ_ACH006.txt
    reason: Missing output files: results/human_heart/GPcounts/FZ_ACH006.csv
    wildcards: dataset=FZ_ACH006
    threads: 10
    resources: tmpdir=/tmp

Activating conda environment: GPcounts-GPU
Will exit after finishing currently running jobs (scheduler).
[Wed Jul  5 16:50:45 2023]
Error in rule run_GPcounts:
    jobid: 4
    input: /data/pinello/PROJECTS/2023_03_SVGBenchmarking/HumanHeart/AnnData/processed_data/FZ_ACH006.h5ad
    output: results/human_heart/GPcounts/FZ_ACH006.csv
    conda-env: GPcounts-GPU
    shell:
        python scripts/real_data/run_GPcounts.py -i /data/pinello/PROJECTS/2023_03_SVGBenchmarking/HumanHeart/AnnData/processed_data/FZ_ACH006.h5ad -o results/human_heart/GPcounts/FZ_ACH006.csv
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Will exit after finishing currently running jobs (scheduler).
Shutting down, this might take some time.
Complete log: .snakemake/log/2023-07-05T160947.107974.snakemake.log
